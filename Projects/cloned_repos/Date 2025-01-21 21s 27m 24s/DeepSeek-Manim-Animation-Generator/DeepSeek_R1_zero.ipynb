{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b88c7b",
   "metadata": {},
   "source": [
    "# **DeepSeek R1-Zero – Quantization & Custom Code Demo**\n",
    "\n",
    "This notebook demonstrates how to load the [DeepSeek R1-Zero model](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero) when it:\n",
    "\n",
    "- Uses **custom code** (model/config) on the Hugging Face Hub.\n",
    "- Potentially uses **4-bit or 8-bit quantization** that requires an updated `bitsandbytes`.\n",
    "- Throws errors about **mismatched model types** or **unsupported quantization**.\n",
    "\n",
    "We'll address the main points:\n",
    "\n",
    "1. **Installing/Upgrading** `transformers`, `accelerate`, and `bitsandbytes`.\n",
    "2. Loading the model with **`AutoModelForSeq2SeqLM`** and **`trust_remote_code=True`**.\n",
    "3. **Disabling** or **overriding** quantization settings if needed.\n",
    "4. **Troubleshooting** common errors (e.g., config mismatch, missing `bitsandbytes` support)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f286b",
   "metadata": {},
   "source": [
    "## **1. Install/Upgrade Dependencies**\n",
    "\n",
    "We'll:\n",
    "- Install or upgrade `transformers`, `accelerate`, and `bitsandbytes`.\n",
    "- Restart the kernel automatically (in some environments) after installation if needed.\n",
    "\n",
    "If you get a prompt asking to trust remote code, it means the model repository has custom Python code you need to run locally. This is normal for custom architectures.\n",
    "\n",
    "**Important**: If you're still seeing an `ImportError` about `bitsandbytes`, ensure the version is at least 0.39.0 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3d8c4",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers accelerate\n",
    "!pip install --upgrade bitsandbytes\n",
    "import sys\n",
    "\n",
    "print(\"Installation complete. If a restart kernel message appears, please restart and re-run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2149df60",
   "metadata": {},
   "source": [
    "## **2. Import Libraries**\n",
    "\n",
    "We’ll rely on the auto-model classes from `transformers` with `trust_remote_code=True`. This allows the custom model code in the `deepseek-ai/DeepSeek-R1-Zero` repository to run, rather than forcing a standard T5.\n",
    "\n",
    "If you want to load in 4-bit mode, make sure your GPU (and `bitsandbytes`) supports it. If you prefer standard float16 or float32, we’ll show how to do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"Imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0d545",
   "metadata": {},
   "source": [
    "## **3. Load the Model & Tokenizer**\n",
    "\n",
    "Below are **two** examples:\n",
    "\n",
    "1. **Full / Half Precision** (float32 or float16), *no quantization.*\n",
    "2. **4-Bit Quantization** (Requires updated `bitsandbytes`).\n",
    "\n",
    "### **(A) Full/Half Precision**\n",
    "If 4-bit is giving trouble or you just want to avoid quantization, load the model in standard float32 or float16:\n",
    "\n",
    "```python\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Zero\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,   # or torch.float32\n",
    "    device_map=\"auto\"          # optional, automatically place model on GPU if available\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e769e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_precision = \"float16\"  # or \"float32\"\n",
    "\n",
    "if model_precision == \"float16\":\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "tokenizer_fp = AutoTokenizer.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Zero\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model_fp = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Zero\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\"  # Places the model on GPU if available, else CPU\n",
    ")\n",
    "\n",
    "print(\"Model loaded in\", model_precision, \"precision.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c9fe1",
   "metadata": {},
   "source": [
    "### **(B) 4-bit Quantization**\n",
    "If you specifically **want** 4-bit quantization, and the config is set up for it, do:\n",
    "\n",
    "```python\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Zero\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "```\n",
    "But you must have an updated version of `bitsandbytes` supporting 4-bit. If it raises an error about `ImportError: Using bitsandbytes 4-bit quantization requires the latest version...`, re-run the earlier installation cell or `pip install --upgrade bitsandbytes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863e0a1",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "# Example code for 4-bit quantization (commented out by default)\n",
    "# Un-comment if you'd like to try 4-bit.\n",
    "\n",
    "use_4bit = False  # Change to True if you want to attempt 4-bit\n",
    "\n",
    "if use_4bit:\n",
    "    tokenizer_4b = AutoTokenizer.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Zero\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model_4b = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Zero\",\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model loaded in 4-bit quantization.\")\n",
    "else:\n",
    "    print(\"4-bit loading is disabled (use_4bit=False). If you want to try it, set use_4bit=True.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22baf71",
   "metadata": {},
   "source": [
    "## **4. Test Inference**\n",
    "\n",
    "We’ll do a simple generation test. If you loaded the float16 model, we’ll test that. If you loaded the 4-bit version, adapt the relevant model/tokenizer references.\n",
    "\n",
    "Feel free to change the prompt to something more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who was the first person to walk on the Moon?\"\n",
    "\n",
    "# We'll use model_fp if we loaded in float16 or float32.\n",
    "# If you loaded 4-bit, swap in model_4b / tokenizer_4b below.\n",
    "\n",
    "inputs = tokenizer_fp(prompt, return_tensors=\"pt\").to(model_fp.device)\n",
    "outputs = model_fp.generate(**inputs, max_length=50)\n",
    "answer = tokenizer_fp.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20202e54",
   "metadata": {},
   "source": [
    "## **5. Additional Troubleshooting**\n",
    "\n",
    "1. **`ImportError: Using bitsandbytes 4-bit quantization requires the latest version of bitsandbytes`**:\n",
    "   - Make sure you ran `!pip install --upgrade bitsandbytes`.\n",
    "   - Restart the runtime if needed, then retry.\n",
    "   - If it still fails, confirm your `bitsandbytes` version is at least 0.39.0+ by checking `bitsandbytes.__version__`.\n",
    "\n",
    "2. **`You are using a model of type deepseek_v3 to instantiate a model of type t5`**:\n",
    "   - This indicates the config is not recognized as T5 internally.\n",
    "   - **Solution**: Use `AutoModelForSeq2SeqLM.from_pretrained(..., trust_remote_code=True)` to let the library load the custom code.\n",
    "   - Avoid forcing `T5ForConditionalGeneration.from_pretrained` on a non-standard T5 model.\n",
    "\n",
    "3. **`ValueError: Unknown quantization type, got fp8...`**:\n",
    "   - The model’s config references `fp8`, which is not widely supported.\n",
    "   - Try specifying your own `torch_dtype` (e.g., `float16`) or disable auto-quantization by removing `load_in_8bit` or `load_in_4bit` arguments.\n",
    "   - Make sure your version of `transformers` and `bitsandbytes` supports the quantization type.\n",
    "\n",
    "4. **If you see**: `Do you wish to run the custom code? [y/N]`\n",
    "   - This is normal for repos with custom architecture. Type `y`, or set `trust_remote_code=True` in your code, so you won’t be prompted again.\n",
    "\n",
    "## **6. Next Steps**\n",
    "With the model loaded, you can:\n",
    "\n",
    "- **Experiment with Prompts**: Try different questions or instructions.\n",
    "- **Adjust Inference Parameters**: `num_beams`, `temperature`, `top_k`, etc.\n",
    "- **Fine-tune**: If you have a custom dataset, you can attempt further training with the Hugging Face [Trainer API](https://github.com/huggingface/transformers).\n",
    "- **Evaluate**: Integrate the model into your workflow for summarization, QA, or general text generation.\n",
    "\n",
    "If you run into further issues, always check:\n",
    "- That your library versions are aligned.\n",
    "- The repository docs on [Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero) for any special instructions.\n",
    "- That you are indeed trusting remote code for custom architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475f779",
   "metadata": {},
   "source": [
    "# **Done!**\n",
    "\n",
    "You now have a notebook that:\n",
    "1. Installs and upgrades required libraries.\n",
    "2. Demonstrates how to load DeepSeek R1-Zero with custom code.\n",
    "3. Explains how to handle quantization errors (`bitsandbytes`).\n",
    "4. Provides a test inference example.\n",
    "\n",
    "Use this as a template for your own experiments or expansions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
